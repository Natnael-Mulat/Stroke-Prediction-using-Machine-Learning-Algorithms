{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "endangered-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC\n",
    "from sklearn.metrics import f1_score, accuracy_score, make_scorer, roc_curve, classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import f1_score, accuracy_score, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,confusion_matrix\n",
    "import pickle\n",
    "\n",
    "data=pd.read_csv('DIG.csv')\n",
    "df=data.copy()\n",
    "df = df.drop(['Unnamed: 0', 'ID'], axis=1)\n",
    "df.head()\n",
    "\n",
    "x = df.drop(['STRK'],axis=1)\n",
    "y=df['STRK']\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.2,shuffle=True,random_state=30)\n",
    "\n",
    "#see data if it is balanced \n",
    "#hint is not balanced \n",
    "\n",
    "#min max scaling \n",
    "# compute statistics on training fold for feature rescaling (min-max)\n",
    "minima = np.min(xtrain, axis=0)\n",
    "maxima = np.max(xtrain, axis=0)\n",
    "\n",
    "# rescale train and test sets\n",
    "xtrain = (xtrain - minima) / (maxima - minima)\n",
    "xtest = (xtest - minima) / (maxima - minima)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dried-freight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO STROKE:  5180\n",
      "STROKE:  260\n",
      "NO STROKE:  2591\n",
      "STROKE:  260\n"
     ]
    }
   ],
   "source": [
    "columns1 = x.columns\n",
    "df_resampling_x = pd.DataFrame(xtrain, columns = columns1)\n",
    "#print(xtrain)\n",
    "df_resampling_y = pd.DataFrame(ytrain, columns = ['id', 'STRK'])\n",
    "del df_resampling_y['id']\n",
    "concat = [df_resampling_x, df_resampling_y]\n",
    "resampled_train_df = pd.concat(concat, axis=1)\n",
    "\n",
    "total = len(resampled_train_df['STRK'])\n",
    "is_stroke = list(resampled_train_df['STRK']).count(1)\n",
    "print(\"NO STROKE: \",total-is_stroke)\n",
    "print(\"STROKE: \",is_stroke)\n",
    "delete_row = resampled_train_df[resampled_train_df[\"STRK\"]==0].index\n",
    "delete_row = delete_row.values[:2589]\n",
    "resampled_train_df= resampled_train_df.drop(delete_row)\n",
    "total = len(resampled_train_df['STRK'])\n",
    "is_stroke = list(resampled_train_df['STRK']).count(1)\n",
    "print(\"NO STROKE: \",total-is_stroke)\n",
    "print(\"STROKE: \",is_stroke)\n",
    "x_train = resampled_train_df.drop(['STRK'],axis=1)\n",
    "y_train = resampled_train_df['STRK']\n",
    "xtrain, x_test,ytrain, y_test = train_test_split(x_train,y_train,test_size=0.0000000000001,shuffle=True,random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quarterly-nigeria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best score: 0.8964296838113548 with param: {'C': 100000, 'class_weight': {0: 1.0, 1: 1.0}}\n",
      "Best score: 0.8964296838113548 with param: {'C': 100000, 'class_weight': {0: 1.0, 1: 1.0}}\n",
      "accuracy_score\n",
      " 0.9911764705882353\n",
      "precision_score\n",
      " 0.9298245614035088\n",
      "recall_score\n",
      " 0.8688524590163934\n",
      "f1_score beta\n",
      " 0.8803986710963455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1295,    4],\n",
       "       [   8,   53]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,confusion_matrix\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "w = [ {0:1.0,1:1.0}]\n",
    "lr_w=LogisticRegression(max_iter=700000)\n",
    "score_f1_beta = make_scorer(fbeta_score, beta=2)\n",
    "grid_values_w = {'class_weight':w, 'C':[ 100000]}\n",
    "grid_lr_acc_w = GridSearchCV(lr_w, param_grid = grid_values_w, cv=5, verbose=10, n_jobs=-1,  scoring = score_f1_beta )\n",
    "grid_lr_acc_w.fit(xtrain, ytrain)\n",
    "grid_lr_acc_w.best_params_\n",
    "import pickle\n",
    "filename = 'LOG50%.sav'\n",
    "pickle.dump(grid_lr_acc_w, open(filename, 'wb'))\n",
    "print(f'Best score: {grid_lr_acc_w.best_score_} with param: {grid_lr_acc_w.best_params_}')\n",
    "#Predict values based on new parameters\n",
    "\n",
    "print(f'Best score: {grid_lr_acc_w.best_score_} with param: {grid_lr_acc_w.best_params_}')\n",
    "lrgpytest_w= grid_lr_acc_w.predict(xtest)\n",
    "print('accuracy_score\\n',accuracy_score(ytest,lrgpytest_w))\n",
    "print('precision_score\\n',precision_score(ytest,lrgpytest_w))\n",
    "print('recall_score\\n',recall_score(ytest,lrgpytest_w))\n",
    "print('f1_score beta\\n',fbeta_score(ytest,lrgpytest_w, beta=2.0))\n",
    "grid_lr_acc_acc_sc=accuracy_score(ytest,lrgpytest_w)\n",
    "grid_lr_acc_pr_sc=precision_score(ytest,lrgpytest_w)\n",
    "grid_lr_acc_rec_sc=recall_score(ytest,lrgpytest_w)\n",
    "grid_lr_acc_f1_sc=f1_score(ytest,lrgpytest_w)\n",
    "confusion_matrix(ytest,lrgpytest_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "chief-crazy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START C=1000, class_weight={0: 1.0, 1: 2.0}, degree=2, kernel=linear\n",
      "[CV 1/5; 1/1] END C=1000, class_weight={0: 1.0, 1: 2.0}, degree=2, kernel=linear; total time=   9.6s\n",
      "[CV 2/5; 1/1] START C=1000, class_weight={0: 1.0, 1: 2.0}, degree=2, kernel=linear\n",
      "[CV 2/5; 1/1] END C=1000, class_weight={0: 1.0, 1: 2.0}, degree=2, kernel=linear; total time=  19.6s\n",
      "[CV 3/5; 1/1] START C=1000, class_weight={0: 1.0, 1: 2.0}, degree=2, kernel=linear\n",
      "[CV 3/5; 1/1] END C=1000, class_weight={0: 1.0, 1: 2.0}, degree=2, kernel=linear; total time=   8.6s\n",
      "[CV 4/5; 1/1] START C=1000, class_weight={0: 1.0, 1: 2.0}, degree=2, kernel=linear\n",
      "[CV 4/5; 1/1] END C=1000, class_weight={0: 1.0, 1: 2.0}, degree=2, kernel=linear; total time=  15.8s\n",
      "[CV 5/5; 1/1] START C=1000, class_weight={0: 1.0, 1: 2.0}, degree=2, kernel=linear\n",
      "[CV 5/5; 1/1] END C=1000, class_weight={0: 1.0, 1: 2.0}, degree=2, kernel=linear; total time=   6.3s\n",
      "accuracy_score\n",
      " 0.9963235294117647\n",
      "precision_score\n",
      " 1.0\n",
      "recall_score\n",
      " 0.9180327868852459\n",
      "f1_score beta\n",
      " 0.9333333333333332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1299,    0],\n",
       "       [   5,   56]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "w = [ {0:1.0,1:2.0}]\n",
    "lsvc_params1  = {\n",
    "    \"kernel\": ['linear'],\n",
    "    \"C\" : [1000],\n",
    "    'class_weight':w,\n",
    "    'degree': [2]\n",
    "}\n",
    "score_f1_beta = make_scorer(fbeta_score, beta=2)\n",
    "lsvc1 =SVC()\n",
    "lsvc_tune1 = GridSearchCV(estimator = lsvc1, param_grid = lsvc_params1, verbose=10, scoring = score_f1_beta, cv = 5, return_train_score = True)\n",
    "lsvc_tune1.fit(xtrain, ytrain)\n",
    "lsvc_tune1.best_params_\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,confusion_matrix\n",
    "scv_1_preds1 = lsvc_tune1.predict(xtest)\n",
    "print('accuracy_score\\n',accuracy_score(ytest,scv_1_preds1))\n",
    "print('precision_score\\n',precision_score(ytest,scv_1_preds1))\n",
    "print('recall_score\\n',recall_score(ytest,scv_1_preds1))\n",
    "print('f1_score beta\\n',fbeta_score(ytest,scv_1_preds1, beta=2.0))\n",
    "grid_lr_acc_acc_sc=accuracy_score(ytest,scv_1_preds1)\n",
    "grid_lr_acc_pr_sc=precision_score(ytest,scv_1_preds1)\n",
    "grid_lr_acc_rec_sc=recall_score(ytest,scv_1_preds1)\n",
    "grid_lr_acc_f1_sc=f1_score(ytest,scv_1_preds1)\n",
    "confusion_matrix(ytest,scv_1_preds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "indie-abortion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START learning_rate=1.5, n_estimators=3000........................\n",
      "[CV 1/5; 1/1] END ......learning_rate=1.5, n_estimators=3000; total time=  21.7s\n",
      "[CV 2/5; 1/1] START learning_rate=1.5, n_estimators=3000........................\n",
      "[CV 2/5; 1/1] END ......learning_rate=1.5, n_estimators=3000; total time=  21.8s\n",
      "[CV 3/5; 1/1] START learning_rate=1.5, n_estimators=3000........................\n",
      "[CV 3/5; 1/1] END ......learning_rate=1.5, n_estimators=3000; total time=  21.7s\n",
      "[CV 4/5; 1/1] START learning_rate=1.5, n_estimators=3000........................\n",
      "[CV 4/5; 1/1] END ......learning_rate=1.5, n_estimators=3000; total time=  21.7s\n",
      "[CV 5/5; 1/1] START learning_rate=1.5, n_estimators=3000........................\n",
      "[CV 5/5; 1/1] END ......learning_rate=1.5, n_estimators=3000; total time=  21.7s\n",
      "Best score: 0.7917091459352509 with param: {'learning_rate': 1.5, 'n_estimators': 3000}\n",
      "accuracy_score\n",
      " 0.9808823529411764\n",
      "precision_score\n",
      " 0.7966101694915254\n",
      "recall_score\n",
      " 0.7704918032786885\n",
      "f1_score beta\n",
      " 0.7755775577557755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1287,   12],\n",
       "       [  14,   47]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "ada_1 = AdaBoostClassifier()\n",
    "ada_1_params  = {\n",
    "     \"n_estimators\":[3000],\n",
    "    \"learning_rate\" : [1.5]\n",
    "}\n",
    "score_f1_beta = make_scorer(fbeta_score, beta=2)\n",
    "ada_1_tune = GridSearchCV(estimator = ada_1, param_grid = ada_1_params, scoring = score_f1_beta, cv = 5, return_train_score = True, verbose = 10)\n",
    "ada_1_tune.fit(xtrain, ytrain)\n",
    "ada_1_tune.best_params_\n",
    "\n",
    "import pickle\n",
    "filename = 'ADA50%.sav'\n",
    "pickle.dump(ada_1_tune, open(filename, 'wb'))\n",
    "print(f'Best score: {ada_1_tune.best_score_} with param: {ada_1_tune.best_params_}')\n",
    "#Predict values based on new parameters\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,confusion_matrix\n",
    "ada_1_preds = ada_1_tune.predict(xtest)\n",
    "print('accuracy_score\\n',accuracy_score(ytest,ada_1_preds))\n",
    "print('precision_score\\n',precision_score(ytest,ada_1_preds))\n",
    "print('recall_score\\n',recall_score(ytest,ada_1_preds))\n",
    "print('f1_score beta\\n',fbeta_score(ytest,ada_1_preds, beta=2.0))\n",
    "grid_lr_acc_acc_sc=accuracy_score(ytest,ada_1_preds)\n",
    "grid_lr_acc_pr_sc=precision_score(ytest,ada_1_preds)\n",
    "grid_lr_acc_rec_sc=recall_score(ytest,ada_1_preds)\n",
    "grid_lr_acc_f1_sc=f1_score(ytest,ada_1_preds)\n",
    "confusion_matrix(ytest,ada_1_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "viral-washington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START class_weight={0: 1.0, 1: 2.0}, criterion=entropy, n_estimators=100\n",
      "[CV 1/5; 1/1] END class_weight={0: 1.0, 1: 2.0}, criterion=entropy, n_estimators=100; total time=   0.6s\n",
      "[CV 2/5; 1/1] START class_weight={0: 1.0, 1: 2.0}, criterion=entropy, n_estimators=100\n",
      "[CV 2/5; 1/1] END class_weight={0: 1.0, 1: 2.0}, criterion=entropy, n_estimators=100; total time=   0.6s\n",
      "[CV 3/5; 1/1] START class_weight={0: 1.0, 1: 2.0}, criterion=entropy, n_estimators=100\n",
      "[CV 3/5; 1/1] END class_weight={0: 1.0, 1: 2.0}, criterion=entropy, n_estimators=100; total time=   0.6s\n",
      "[CV 4/5; 1/1] START class_weight={0: 1.0, 1: 2.0}, criterion=entropy, n_estimators=100\n",
      "[CV 4/5; 1/1] END class_weight={0: 1.0, 1: 2.0}, criterion=entropy, n_estimators=100; total time=   0.6s\n",
      "[CV 5/5; 1/1] START class_weight={0: 1.0, 1: 2.0}, criterion=entropy, n_estimators=100\n",
      "[CV 5/5; 1/1] END class_weight={0: 1.0, 1: 2.0}, criterion=entropy, n_estimators=100; total time=   0.6s\n",
      "Best score: 0.5322026488774049 with param: {'class_weight': {0: 1.0, 1: 2.0}, 'criterion': 'entropy', 'n_estimators': 100}\n",
      "accuracy_score\n",
      " 0.9794117647058823\n",
      "precision_score\n",
      " 1.0\n",
      "recall_score\n",
      " 0.5409836065573771\n",
      "f1_score beta\n",
      " 0.5956678700361011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1299,    0],\n",
       "       [  28,   33]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "rfc_1 = RandomForestClassifier()\n",
    "rfc_1_params  = {\n",
    "    \"n_estimators\":[100],\n",
    "    \"criterion\" : ['entropy'],\n",
    "    'class_weight': [{0:1.0,1:2.0}]\n",
    "}\n",
    "score_f1_beta = make_scorer(fbeta_score, beta=2)\n",
    "rfc_1_tune2 = GridSearchCV(estimator = rfc_1, param_grid = rfc_1_params, cv = 5, scoring=score_f1_beta, return_train_score = True, verbose = 10)\n",
    "rfc_1_tune2.fit(xtrain, ytrain)\n",
    "rfc_1_tune2.best_params_\n",
    "import pickle\n",
    "filename = 'RFC50%.sav'\n",
    "pickle.dump(rfc_1_tune2, open(filename, 'wb'))\n",
    "print(f'Best score: {rfc_1_tune2.best_score_} with param: {rfc_1_tune2.best_params_}')\n",
    "#Predict values based on new parameters\n",
    "rfc_1_preds = rfc_1_tune2.predict(xtest)\n",
    "print('accuracy_score\\n',accuracy_score(ytest,rfc_1_preds))\n",
    "print('precision_score\\n',precision_score(ytest,rfc_1_preds))\n",
    "print('recall_score\\n',recall_score(ytest,rfc_1_preds))\n",
    "print('f1_score beta\\n',fbeta_score(ytest,rfc_1_preds, beta=2.0))\n",
    "grid_lr_acc_acc_sc=accuracy_score(ytest,rfc_1_preds)\n",
    "grid_lr_acc_pr_sc=precision_score(ytest,rfc_1_preds)\n",
    "grid_lr_acc_rec_sc=recall_score(ytest,rfc_1_preds)\n",
    "grid_lr_acc_f1_sc=f1_score(ytest,rfc_1_preds)\n",
    "confusion_matrix(ytest,rfc_1_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aging-corruption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START activation=tanh, alpha=1e-05, learning_rate=adaptive, solver=adam\n",
      "[CV 1/5; 1/1] END activation=tanh, alpha=1e-05, learning_rate=adaptive, solver=adam; total time=  11.7s\n",
      "[CV 2/5; 1/1] START activation=tanh, alpha=1e-05, learning_rate=adaptive, solver=adam\n",
      "[CV 2/5; 1/1] END activation=tanh, alpha=1e-05, learning_rate=adaptive, solver=adam; total time=  11.5s\n",
      "[CV 3/5; 1/1] START activation=tanh, alpha=1e-05, learning_rate=adaptive, solver=adam\n",
      "[CV 3/5; 1/1] END activation=tanh, alpha=1e-05, learning_rate=adaptive, solver=adam; total time=  11.7s\n",
      "[CV 4/5; 1/1] START activation=tanh, alpha=1e-05, learning_rate=adaptive, solver=adam\n",
      "[CV 4/5; 1/1] END activation=tanh, alpha=1e-05, learning_rate=adaptive, solver=adam; total time=  11.7s\n",
      "[CV 5/5; 1/1] START activation=tanh, alpha=1e-05, learning_rate=adaptive, solver=adam\n",
      "[CV 5/5; 1/1] END activation=tanh, alpha=1e-05, learning_rate=adaptive, solver=adam; total time=  10.6s\n",
      "accuracy_score\n",
      " 0.986764705882353\n",
      "precision_score\n",
      " 0.9215686274509803\n",
      "recall_score\n",
      " 0.7704918032786885\n",
      "f1_score beta\n",
      " 0.7966101694915254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1295,    4],\n",
       "       [  14,   47]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "score_f1_beta = make_scorer(fbeta_score, beta=2)\n",
    "parameter_space = {\n",
    "    'activation': ['tanh'],\n",
    "    'solver': [ 'adam'],\n",
    "    'alpha': [.00001],\n",
    "    'learning_rate': ['adaptive'],\n",
    "}\n",
    "score_f1_beta = make_scorer(fbeta_score, beta=2)\n",
    "mlp_gs = MLPClassifier(max_iter=10000)\n",
    "clf = GridSearchCV(mlp_gs, parameter_space, cv=5, scoring=score_f1_beta, verbose=10)\n",
    "clf.fit(xtrain, ytrain)\n",
    "clf.best_params_\n",
    "import pickle\n",
    "filename = 'MLP50%.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "clf_preds = clf.predict(xtest)\n",
    "print('accuracy_score\\n',accuracy_score(ytest,clf_preds))\n",
    "print('precision_score\\n',precision_score(ytest,clf_preds))\n",
    "print('recall_score\\n',recall_score(ytest,clf_preds))\n",
    "print('f1_score beta\\n',fbeta_score(ytest,clf_preds, beta=2.0))\n",
    "grid_lr_acc_acc_sc=accuracy_score(ytest,clf_preds)\n",
    "grid_lr_acc_pr_sc=precision_score(ytest,clf_preds)\n",
    "grid_lr_acc_rec_sc=recall_score(ytest,clf_preds)\n",
    "grid_lr_acc_f1_sc=f1_score(ytest,clf_preds)\n",
    "confusion_matrix(ytest,clf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "typical-demonstration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_ROC log:  0.9328865836267493\n",
      "AUC_ROC ada:  0.8806269639950025\n",
      "AUC_ROC svc:  0.959016393442623\n",
      "AUC_ROC rfc:  0.7704918032786885\n",
      "AUC_ROC mlp:  0.883706255757897\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"AUC_ROC log: \", roc_auc_score(ytest, lrgpytest_w))\n",
    "print(\"AUC_ROC ada: \", roc_auc_score(ytest, ada_1_preds))\n",
    "print(\"AUC_ROC svc: \", roc_auc_score(ytest, scv_1_preds1))\n",
    "print(\"AUC_ROC rfc: \", roc_auc_score(ytest, rfc_1_preds))\n",
    "print(\"AUC_ROC mlp: \", roc_auc_score(ytest,clf_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-drinking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
